[
  {
    "objectID": "TR - Yahtzee MCTS.html",
    "href": "TR - Yahtzee MCTS.html",
    "title": "Title",
    "section": "",
    "text": "Lightweight MCTS-based autonomous agent for Yahtzee"
  },
  {
    "objectID": "TR - Yahtzee MCTS.html#mcts-algorithm",
    "href": "TR - Yahtzee MCTS.html#mcts-algorithm",
    "title": "Title",
    "section": "MCTS algorithm",
    "text": "MCTS algorithm\nMonte Carlo Tree Search (MCTS) is a model-free, online planning method. The algorithm explores the decision space by building a search tree in a sequential manner. MCTS involve four phases in its execution, that are repeated until the computational budget is exhausted. The state-action space and the transition probability matrix are represented under the form of a graph.\n\nSelection\n\nStarting from the root node, which is the current state of the MDP, the algorithm select child nodes in the tree until a leaf node is reached. The child selection is performed according to a selection policy. In our case we use the well-known UCT selection policy \\[\\arg \\max_{a \\in A(s)}\\bigg(\\bar{X}(s,a) + C_p\\sqrt{\\frac{log(N(s))}{N(s,a)}}\\bigg)\\] Where :\n\n\\(\\bar{X}(s,a)\\) the average reward after taking action \\(a\\) in state \\(s\\)\n\\(N(s)\\) the number of visits to state \\(s\\)\n\\(N(s,a)\\) the number of times action \\(a\\) has been selected from state \\(s\\)\n\\(C\\) the exploration parameter (can be adjusted to encourage/discourage exploration)\n\n\nExpansion\n\nIf the selected node is a non-terminal state and not fully expanded (meaning not all possible actions have been explored), a child node is added to the tree, representing one possible future states \\(s'\\) resulting from an available action \\(a \\in A(s)\\). We then switch to the child node and repeat the expansion procedure.\n\nSimulation (rollout)\n\nFrom the child node, simulate random trajectories using a default policy (in our case, a random action selection policy) until a terminal state is reached.\n\nBackpropagation\n\nAfter the simulation is done, the reward obtained is propagated back up the tree to update the nodes and actions involved in the selection and expansion phases.\nThe updates are made using the following steps for each node visited in the path from the leaf node to the root :\n\nIncrement the visit count \\(N(s,a)\\) for the action \\(a\\) taken at state \\(s\\)\nUpdate the average reward \\(\\bar{X}(s,a)\\) for the action \\(a\\) at state \\(s\\) as follows : \\[\\bar{X} (s,a) \\leftarrow \\bar{X}(s,a) + \\frac{1}{N(s,a)}\\big(r - \\bar{X}(s,a)\\big)\\]\n\n\nAction selection\n\nOnce we have exhausted our computational budget the simulation stops and we select the action that have the highest value \\[\\arg \\max_{a\\in A(s)} \\bar{X}(s_0,a)\\]"
  },
  {
    "objectID": "TR - Yahtzee MCTS.html#implementation",
    "href": "TR - Yahtzee MCTS.html#implementation",
    "title": "Title",
    "section": "Implementation",
    "text": "Implementation\nThe game engine and the Monte Carlo Tree Search (MCTS) algorithm for Yahtzee have been developed in both Python and JavaScript to provide flexibility in deployment and testing. The Python version was used primarily during the research phase, while the JavaScript version was developed later, for integration into the web application.\nBoth implementations are open-source, the link to the repository containing their respective source code can be found in Appendix.\nBoth of them are similar in terms of logic and sequence only minor differences due to language-specific features and integration need (i.e.: using web workers for MCTS computation in JS version to avoid blocking the UI). In this section we will discuss mainly of the JavaScript implementation and its integration in the web application while in the Results section describe experiments conducted using the Python version.\nFeatures\nThe web application have two modes : AI opponent and AI assistant. In AI opponent mode, the player face the AI agent in a game of Yahtzee. In AI assistant mode, the player plays alone but benefits from advices coming from the AI agent.\nAI assistant provide the player with information such as:\n\nThe mean state-action value \\(\\hat{X}(s,a)\\)\nThe standard error of the mean state-action value\nThe sample standard deviation of the state-action values\nThe sample size \\(n\\), corresponding number of time this action has been sampled during MCTS\nAn histogram of the expected final score conditioned on the current state \\(s\\) and the selected action \\(a\\)\n\nNote : The mean state-action value here is conditioned to following the policy determined by the MCTS algorithm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bienvenue !",
    "section": "",
    "text": "Je m’appelle Yacine BEKKA, bienvenue sur mon site personel !\nJe suis un expert en gestion des données, diplômé de l’Université du Colorado en data science. Au cours des huit dernières années, j’ai accompagné diverses organisations dans la création de valeur ajoutée grâce à une meilleure gestion de leurs données. J’ai travaillé sur des sujets variés tels que la mise en place de logiciels de gestion des données de référence (MDM), la gestion de la qualité des données, la gouvernance des données, et bien d’autres, au sein de multiples secteurs d’activité (web, assurances, banque d’affaires, agroalimentaire, ferroviaire, etc.).\nFort de ma formation et de mes expériences professionnelles, j’offre des services de conseil aux organisations, principalement dans les domaines de la gouvernance des données, de la gestion des données, de l’architecture des données, et de la gestion de la qualité des données.\nJe publie du contenu original que vous pouvez retrouver ici et sur mes différents réseaux sociaux. Dans mon temps libre, je travaille également sur divers projets (logiciels open source, contenu long-format, recherche), que vous pouvez également consulter sur ce site.\n\n\n\n\nMise en ligne du blog ! (09/2024)\nMise en ligne du projet “Agent autonome pour le jeu de Yathzee” (09/2024)\nGuide to Data ROI 2e édition, en cours d’écriture (2024/2025)"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Bienvenue !",
    "section": "",
    "text": "Je m’appelle Yacine BEKKA, bienvenue sur mon site personel !\nJe suis un expert en gestion des données, diplômé de l’Université du Colorado en data science. Au cours des huit dernières années, j’ai accompagné diverses organisations dans la création de valeur ajoutée grâce à une meilleure gestion de leurs données. J’ai travaillé sur des sujets variés tels que la mise en place de logiciels de gestion des données de référence (MDM), la gestion de la qualité des données, la gouvernance des données, et bien d’autres, au sein de multiples secteurs d’activité (web, assurances, banque d’affaires, agroalimentaire, ferroviaire, etc.).\nFort de ma formation et de mes expériences professionnelles, j’offre des services de conseil aux organisations, principalement dans les domaines de la gouvernance des données, de la gestion des données, de l’architecture des données, et de la gestion de la qualité des données.\nJe publie du contenu original que vous pouvez retrouver ici et sur mes différents réseaux sociaux. Dans mon temps libre, je travaille également sur divers projets (logiciels open source, contenu long-format, recherche), que vous pouvez également consulter sur ce site."
  },
  {
    "objectID": "index.html#mon-actu",
    "href": "index.html#mon-actu",
    "title": "Bienvenue !",
    "section": "",
    "text": "Mise en ligne du blog ! (09/2024)\nMise en ligne du projet “Agent autonome pour le jeu de Yathzee” (09/2024)\nGuide to Data ROI 2e édition, en cours d’écriture (2024/2025)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Présentation : Le problème des stratégies data en 2022\n\n\n1 min\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData management buzzword\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData quality rules\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat NOT to do for improving data quality\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to explain the value of data to your colleagues\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData VS Information\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData management jargon\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData in PR vs Data in Reality\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPersuade your boss/team about the value of your projects\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProblème de qualité de données et désastre opérationel à la NASA\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLa data gouvernance comme les antibiotiques, c’est pas automatique !\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLe pire ennemi du professionel de la data\n\n\n3 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData quality target level\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContrôle de qualité et problèmes de qualités de données\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions de la qualités de données\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC’est quoi la data gouvernance ?\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStratégie data : Objectifs vs Moyens\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData flow map\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nC’est quoi un data manager ?\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCréer de l’intérêt pour un projet data\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nL’importance de la clarté dans les projets data\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData quality dimensions\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContrôle de qualité de données déterministe vs probabiliste\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nL’importance de s’attaquer au causes racines de la non-qualité\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4 livres à lire sur la gestion de la qualité de données\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL’iceberg de la data quality\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3 définitions pour mieux gérer ses données d’entreprises\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n4 frameworks de gestion de la qualité de données à connaitre !\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 frameworks de gestion de la qualité de données à connaitre !\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSix sigma et qualité de données, l’erreur classique\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n3 méthodes pour calculer un indice de qualité des données\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDialogue fictif entre un CDO et un CEO…\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData quality management process\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust but verify\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nValeur financière d’une données\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStandard de qualité des données\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMeme business case quantitatif\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nModel-as-a-service\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nValorisation des données\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPrévalence, détection et faux positif\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData-as-an-asset\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarousel KPI Tree\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarousel VEIPPP\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness drivers of data quality management\n\n\n3 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarousel VEII\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarousel VEIP\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarousel Espérance et decision\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCarousel Simulation monte carlo\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSlides - Data quality introduction\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSlides - Démarche de gestion de la qualité des données\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSlides - Standard de qualité des données\n\n\n1 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEtude de cas : Gestion de la qualité de données chez le Groupe SeLoger\n\n\n9 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDealing with duplicates\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRules-based DQ control vs ML/AI based DQ control\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n5 practical tips for better quality management\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData reliability\n\n\n2 min\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/post_8.html",
    "href": "blog/post_8.html",
    "title": "5 practical tips for better quality management",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-datastrategy-datamanagement-activity-7026086062946963456-FgQ3?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 31 Jan 2023\n\nGet your data quality under control with those 5 practical tips ⬇️\n➡ Create the business case for hashtag#dataquality\nData quality is not a goal by itself, it is a means to an end. To convince management and ensure their support you need to demonstrate why data quality matters for the company and how it will contribute to the business strategy.\nWhile it can be difficult to show a quantitative ROI at the start, you can always start by a qualitative business case (check out my post on 4 business drivers of data quality here : https://lnkd.in/eYixnUyu)\n➡ Start by known pain points\nTargeting known pain points is an easy way to create adherence around your data quality management process.\nIf by the implementation of the process you are able to solve or at least draw a clear view of the root causes, the effort for correction and the business impacts, you will demonstrate the added value of data quality to your company.\n➡ Iterate implementation by business terms\nIf you cannot implement data quality monitoring for all your data (which is often the case), I recommend working by iteration, defining the scope by business terms.\nIn my experience, it is the best way to scope your iterations based on company priority as business terms are easily relatable and can be linked to known pain points and company strategy\n➡ Evaluate criticality and business impact of issues not controls\nNo one cares about the data quality controls, everybody cares about data quality issues. In most of the cases controls are quite cheap to perform in terms of money and effort.\nWhat is costly is the resolution of data quality issues, it is also where the business impact(s) can be evaluated more precisely\n➡ Communicate regularly, show the value\nRegular exchange on the current level of data quality, data quality issues and their resolution should be done with data consumers, data producers and business SMEs.\nOnce you have some, data quality success stories should be shared as much as possible in the company, especially to management. Don’t hesitate to do the “promotion” of data quality management and the added value that it brings."
  },
  {
    "objectID": "blog/post_6.html",
    "href": "blog/post_6.html",
    "title": "Dealing with duplicates",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-datamanagement-duplicate-activity-7021904253229137921-NYTk?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 19 Jan 2023\n\nWe have all encountered duplicate hashtag#data in the course of our careers, let’s see how we can deal with it ⬇️\n🤔 What is a duplicates\nIn the context of enterprise hashtag#datamanagement, we can think of hashtag#duplicate data as a redundant representation of a business object (customer, invoice, material, etc.). An example with a customer referential might look like this :\nCustomer ID = 1 -&gt; Customer Name = Jane, Doe -&gt; Customer Status = Active Customer ID = 365 -&gt; Customer Name = Jan Doe -&gt; Customer Status = Payment Default Customer ID = 1455 -&gt; Customer Name = JaneDoe -&gt; Customer Status = Active\n🔎 Identify duplicates\nTo ensure that our data is of high quality, we need to make sure that there is no redundant representation, especially when there is conflicting information. The main difficulty is that duplicates can be hard to identify depending on context and initial data quality.\nIn some cases, a simple deterministic check (whether records are duplicates for sure) may suffice, e.g., an exact match on all or a subset of attributes. In other cases, probabilistic checking (is it likely that these records are duplicates) is required (string edit distance comparison, phonetic comparison, combination of multiple methods, etc.).\n🚧 Prevent duplicates\nUsing a unique identifier for our business objects is the first step in properly managing duplicates. Even though the unique identifier ID does not directly prevent the creation of duplicates, it enables unique identification of records, which is a prerequisite for identifying duplicates.\nAn efficient way to prevent duplicates is to include a “matching” step (which can be manual or automated) in the creation process.\n🪛 Correct duplicates\nCorrecting duplicates is a complex process and requires several steps : - Identification (match duplicate records) - Grouping (cluster duplicates together) - Merging (keep the fittest record)\nDepending on the data and context, you may also need to manage the transfer of other business objects linked with the deduplicated business object. Full traceability of the correction process is also a best practise.\nhashtag#dataquality"
  },
  {
    "objectID": "blog/post_57.html",
    "href": "blog/post_57.html",
    "title": "Etude de cas : Gestion de la qualité de données chez le Groupe SeLoger",
    "section": "",
    "text": "Date originale de publication : 31 mars 2022"
  },
  {
    "objectID": "blog/post_57.html#introduction",
    "href": "blog/post_57.html#introduction",
    "title": "Etude de cas : Gestion de la qualité de données chez le Groupe SeLoger",
    "section": "Introduction",
    "text": "Introduction\nGroupe Seloger est le groupe leader des portails immobiliers en France. Le groupe possède notamment les marques SeLoger, LogicImmo, Meilleurs Agents, etc\nLa qualité des données est très importante pour le groupe étant donné que leur business model est étroitement lié aux données. En tant que portail d’annonces immobilières, il est d’une importance majeur de servir des données de la plus haute qualité au utilisateurs du portail, sur les biens disponibles mais également pour leurs clients du groupe (principalement des agences immobilières), de pouvoir fournir des statistiques précises et de qualité sur les interactions des utilisateurs sur les annonces. La qualité des données est également un facilitateur majeur d’innovation pour le groupe car elle impacte la capacité à développer des nouveaux produits/fonctionnalités telle qu’un moteur de recommandations basé sur les préférences des utilisateurs.\nNotre mission consiste à implémenter, en partant de zéro, un processus de gestion de la qualité des données pour la plateforme de données du groupe (hébergée sur Amazon AWS) en collaboration avec les équipes du département Data."
  },
  {
    "objectID": "blog/post_57.html#challenges",
    "href": "blog/post_57.html#challenges",
    "title": "Etude de cas : Gestion de la qualité de données chez le Groupe SeLoger",
    "section": "Challenges",
    "text": "Challenges\n\nImportant volume et grande variété de données collectées chaque jour dans la plateforme de données (150+ jeux de données allant de 10-15M à 20G entrées, allant du référentiel clients au interactions des utilisateurs sur les portails)\nFaible maturité et culture sur la gouvernance des données et la gestion de la qualité des données dans le groupe\nFaible implication initiales du “métiers”, interlocuteurs venant principalement des équipes IT/Data\nLes données de la plateforme sont utilisées par différents “consommateurs” internes pour des usages variés, définition de la qualité variant également selon les usages/interlocuteurs\nPas de ressources dédiés à la gestion de la qualité de données, nécessité de s’appuyer sur les équipes et outils existants\nDû à l’important volume de données et à l’infrastructure technique, la reprise de données ainsi que la remédiation des causes racines peut nécessiter des coûts et des efforts élevés"
  },
  {
    "objectID": "blog/post_57.html#solution",
    "href": "blog/post_57.html#solution",
    "title": "Etude de cas : Gestion de la qualité de données chez le Groupe SeLoger",
    "section": "Solution",
    "text": "Solution\nAfin de dépasser ces difficultés et livrer les résultats demandés, nous avons utilisé le Data Quality Launchpad, notre méthodologie pour accélérer votre transformation digitale et réduire vos déchets opérationnels en rendant votre organisation autonome pour gérer et améliorer de façon continue la qualité de vos données.\n\nIdentify\n\nPour cette première étape, nous échangeons avec les équipes data pour définir ensemble quelles sont les exigences en termes de qualité de données. Etant donné le nombre élevé de jeux de données à couvrir et le fait que nous n’ayons pas accès à des interlocuteurs “métiers” nous avons utilisé une approche de type générique/spécifique pour la définition des standards de qualité de données (un standard de qualité de données est la collection des règles applicable à un jeux de données)\nLe standard de qualité générique est conçu pour être applicable à l’ensemble des jeux de données et les standards spécifiques sont eux dédiés à un jeux de données en particulier. Cette approche nous permet de couvrir l’ensemble des jeux de données avec le standard générique mais avec une précision amoindri (dû à la généricité du standard) et un effort de spécification faible alors que les standards spécifiques sont plus précis mais nécessite plus d’implication des interlocuteurs “métiers”\nUne fois les standards de qualité définis et formalisés, nous dérivons ensuite les indicateurs de qualité associés et nous effectuons les mesures initiales de qualité. Nous avons ensuite analysé et présenté ces mesures au “data consumers” (collaborateurs utilisant ce jeux de données en particulier). Comme dans la plupart des organisations, les problèmes de qualité de données sont souvent cachés ou mal définis. Avec cette première étape nous avons découvert plusieurs problèmes de qualité à fort impact que nous avons présentés de façon factuel. Les “data consumers” ont été très surpris par certaines de ces découvertes et nous ont aidés à évaluer les impacts, prioriser les différents problèmes et générer de l’intérêt autour de la qualité de données.\n\nImprove\n\nDans le Groupe SeLoger, les équipes data travaillent de manière très autonome, en suivant la méthode Agile “by-the-book”. Afin de s’appuyer sur les ressources, les outils et les modes de travail déjà en place dans le groupe, nous avons intégré les différentes activités de notre méthodologie de gestion de la qualité de données avec le processus actuel de gestion des données du groupe en répartissant les responsabilités sur les rôles existants. Nous accompagnons et aidons également à l’adoption de ces nouvelles tâches auprès des différents collaborateurs via des sessions de formation et la rédaction de procédures opérationnelles normalisées.\nUne fois les rôles et responsabilités définis, nous nous sommes concentrés sur l’investigation des causes racines pour les problèmes de data quality ayant la priorité la plus élevés. Pour cela, nous commençons par lister les différentes hypothèses de causes racine et nous vérifions et testons ensuite la validité de ces hypothèses par ordre de probabilité. Ce travail nécessite l’implication de plusieurs équipes à l’intérieur et à l’extérieur du departement Data. L’intérêt généré par la découvertes de ces problèmes nous a aidé à impliquer ces différentes équipes, nous avons aussi bien distinguer la phase d’identification des causes racines et leur résolution, en nous concentrant d’abord uniquement sur l’identification.\nAprès avoir déterminé les causes racines des problèmes de qualité à haute priorité, nous avançons vers la phase de solution et d’analyse de coûts-bénéfices afin d’arbitrer s’il est pertinent de corriger la cause racine ou non. Dans cette étude de cas, certains problèmes de qualité avaient un tel impact que les bénéfices dépassaient largement les coûts mais pour d’autres ce n’était pas le cas. Dans cette situation, nous avons définis des solutions alternatives qui, bien que non-optimales, permettent de prévenir les futurs problèmes autant que possible.\n\nCommunicate\n\nPour la communication, nous concentrons nos efforts sur deux axes:\nCommunication sur les aspects opérationnels (niveaux actuels de qualités de données, statut de résolution des problèmes de qualité, alertes de la part des “consommateurs” de données) via les notice de qualité de données\nCommunication au management et de façon plus large, à l’ensemble de la société (“success stories” de la qualité de données) via la Data Quality Newsletter\nPour les notices de qualité de données nous publions des notices, disponibles pour l’ensemble des collaborateurs utilisant ces données. Pour cela nous utilisons Confluence (l’outil utilisé par les équipes Data pour leur site web interne contenant la documentation relatives aux jeux de données) ainsi que les démo de sprint. De cette manière nous pouvons partager les informations opérationnelles sur la qualité des données et recevoir des feedbacks de la part des collaborateurs en “live” et sur “papier”.\nPour la Newsletter, nous utilisons Slack étant donné que c’est l’outil de communication principal au sein du groupe. Les différences avec les notices de qualité sont le contenu (moins opérationnels, plus orienté “bénéfices”), la périodicité de publication (newsletter est publié moins souvent que les notices) ainsi que le format (plus court et moins exhaustif que les notices)\n\nIterate\n\nA ce stade, l’organisation est déjà autonome en termes de gestion de la qualité de données, la mission s’oriente donc vers l’amélioration continue et la gestion de la qualité des données sur le long-terme\nGrâce au gain de maturité et l’intérêt généré autour de la gestion de la qualité de données nous avons eu une plus grande implication du management et des collaborateurs “métiers”. Ceci nous a permis de retravailler certains des standards de qualité afin de les rendre encore plus précis, ce qui à ensuite mené à la découverte de nouveaux problèmes de qualité.\nPour ces nouveaux problèmes, nous suivons donc le processus de gestion de la qualité maintenant en place dans le groupe (décrit dans l’étape “Improve”)\nNous industrialisons également le suivi des indicateurs de qualités de données en développant un outil Python pour formaliser les standards de qualité, calculer les indicateurs associés et générer un tableau de bord récapitulatif"
  },
  {
    "objectID": "blog/post_57.html#résultats-obtenus",
    "href": "blog/post_57.html#résultats-obtenus",
    "title": "Etude de cas : Gestion de la qualité de données chez le Groupe SeLoger",
    "section": "Résultats obtenus",
    "text": "Résultats obtenus\n\nMeilleure visibilité sur les problèmes de qualité de données et les impacts métiers associés\nÉvaluation factuelle du niveau actuel de qualité des données\nDéfinition de la qualité claire et partagée dans l’organisation (“producteur” de données, équipe data et “consommateurs” de données)\nProcessus systématique pour résoudre les problèmes de qualité de données en utilisant les outils et ressources existantes\nOutil configurable et facile d’utilisation pour suivre l’évolution de la qualité des données et générer des graphiques\nAmélioration de la qualité des données pour les jeux de données les plus important de la plateforme (interaction utilisateurs et association aux annonces immobilières, adresse-mail utilisateurs et association au critères de recherches)\nRapport réguliers et publication de notice de qualité de données pour partager les niveaux actuels de qualité de données ainsi que le statut de résolution des problèmes identifiés dans l’organisation\nMeilleur utilisations des ressources de l’équipe data grâce à la définition claire des rôles et responsabilités (plus de nettoyage de données “caché”)\nConfiance accru dans la qualité des données au sein de l’organisation\nAutonomie dans la gestion et l’amélioration continue de la qualité des données\n\n\nTémoignages\n“Alors que nous étions au niveau 0 de maturité sur les sujets Data Quality au sein du pôle data du Groupe Se Loger, Yacine a su s’adapter à notre contexte et à nos besoins afin de nous faire progresser.\nTrès autonome, mais systématiquement dans la collaboration, il m’a impressionnée par sa rapidité à comprendre notre contexte et à s’adapter au mieux à nos besoins. Ce n’est pas quelqu’un qui abandonne au premier obstacle, on peut lui faire confiance pour aller au bout des choses.\nJ’ai apprécié sa force de proposition, sa proactivité et sa pédagogie, qui nous ont aidé à embarquer les parties prenantes sur l’initiative. En plus de ça c’est quelqu’un qui fait preuve d’une grande humilité, avec un bon esprit d’équipe. En bref, je ne peux que le recommander et serai ravie de collaborer à nouveau avec lui.”\nEmanuelle Galet, Product Owner Data chez Groupe Seloger"
  },
  {
    "objectID": "blog/post_55.html",
    "href": "blog/post_55.html",
    "title": "Slides - Démarche de gestion de la qualité des données",
    "section": "",
    "text": "Vidéo Youtube : https://youtu.be/SIf8w5Fx6Cc\nDate originale de publication : 28 May 2023"
  },
  {
    "objectID": "blog/post_53.html",
    "href": "blog/post_53.html",
    "title": "Carousel Simulation monte carlo",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_simulation-monte-carlo-activity-7075019156529537024-KHvE?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Thu, 15 Jun 2023"
  },
  {
    "objectID": "blog/post_51.html",
    "href": "blog/post_51.html",
    "title": "Carousel VEIP",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_valeur-esp%C3%A9r%C3%A9e-dune-information-parfaite-activity-7077518067353665536-UoTN?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Thu, 22 Jun 2023"
  },
  {
    "objectID": "blog/post_5.html",
    "href": "blog/post_5.html",
    "title": "Business drivers of data quality management",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_4-business-drivers-of-data-quality-activity-7020705759898292224–ObA?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 16 Jan 2023\n\nWhy does your organization need data quality in the first place ? Let me introduce you to 4 business drivers of data quality management 👇\n🎯 Operational excellence\nImproved hashtag#dataquality leads to more efficiency in your company’s operations.\nIn a multinational company, data often needs to be collected from multiple sources (ERP, legacy, financial/accounting applications, etc.) for balance sheet calculations. With low data quality, this process becomes a nightmare as you have to check for duplicates, inconsistencies between sources, missing data, anomalies, etc., causing additional costs and delays.\n🧠 Better decision making\nHigh-quality data provided in a reliable manner helps in decision making by providing trusted insights and information about the company’s business.\nThe commercial department of a multinational company decides to perform a simple analysis to find out which customer account accounts for the largest share of revenues. The invoices data are stored in ERP, while the customers (from the trade department’s point of view) are stored in CRM. If the DQ is low, mapping CRM customers to ERP customers could be difficult (major customer LTD vs. major customer UK vs. major customer). In this case, some of the invoices made with this customer might not be included in the final analysis, which would affect the decisions based on this analysis.\n🚀 Enable innovation through hashtag#analytics and hashtag#machinelearning\nTo quote SCOTT TAYLOR - The Data Whisperer, the golden rule for data is “garbage in, garbage out.” Low data quality can prevent companies from leveraging AI and machine learning to stay ahead of the competition.\nExample : an automotive manufacturer wants to develop a new predictive maintenance service (parts replaced just-in-time before of failure), but when they review their service order database, they find that there are a large number of duplicates and missing data. If they use this data “as is” to train an hashtag#AI algorithm, the results will not be very efficient, and likely this service will not see the light of day.\n💵 Monetization of data\nIn the age of personalized advertising and artificial intelligence, data is now a commodity that can have solid financial value if managed properly. As a business, you generate data and have access to some data sets that are likely unique or very difficult to replicate. If you can provide this regularly and reliably in high quality, it can be a new revenue stream.\nExample: a stockbroker has access to the order flow of his customers. This data is of great value as it can reveal who is buying/selling a particular stock or financial instrument at which moment, and can then be sold to third parties or used by the company itself (subject to laws and regulations). On the other hand, if the data quality is low (duplicate customers, wrong ISIN code/ticker, wrong timestamp, etc.), the value of this data set decreases or even goes to zero…"
  },
  {
    "objectID": "blog/post_48.html",
    "href": "blog/post_48.html",
    "title": "Carousel KPI Tree",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_carousel-kpi-tree-activity-7081504356298027008-brgE?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 03 Jul 2023"
  },
  {
    "objectID": "blog/post_46.html",
    "href": "blog/post_46.html",
    "title": "Prévalence, détection et faux positif",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_petite-%C3%A9nigme-pour-la-team-linkedin-un-logiciel-activity-7080424719635607552-UpMc?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Fri, 30 Jun 2023\n\nPetite énigme pour la team Linkedin\nUn logiciel de data quality, basé un algorithme de détection d’anomalies, permettant d’identifer les problème de qualité est à l’étude….\nSuite à des test sur des données historiques, on sait les choses suivantes :\n\nSur 10 anomalies, 9 sont marquées par le logiciel\nSur 10 entrées correctes, 2 sont marquées comme anomalies\n\nLes affirmations ci-dessous sont-elles vraies ? Et pourquoi ?\n\nSi une entrée est marquée par le logiciel il y a 90% de probabilité que ce soit une anomalie.\nUne entrée correcte à 80% de probabilité de ne pas être marquée comme anomalie."
  },
  {
    "objectID": "blog/post_44.html",
    "href": "blog/post_44.html",
    "title": "Model-as-a-service",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataasset-datamanagement-datastrategy-activity-7079337576704159744-KK3n?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 27 Jun 2023\n\nOpenAI, un des meilleurs business modèle au monde ?\nLe “Model-as-a-service”\nOpenAI loue l’accès à leur modèles pour faire des revenues\nMais à la différence de louer un bien immobilier ou une voiture\nLes modèles d’OpenAI eux : - peuvent être loué à plusieurs clients en même temps - Plus ils sont utilisés plus ils vont s’apprécier\nOpenAI est doublement gagnant, les utilisateurs payent pour l’accès et par leur utilisation, contribuent à améliorer les modèles\nD’autres sociétés comme Tesla, Google, etc ne donnent pas accès directement au modèle mais vendent un produits ou un service “on-top” et bénéficie du même effet “kiss-cool”\nAutre points à noter :\n\nCes actifs ne sont pas inscrits au bilan comptable donc pas taxés directement\nPlus ils s’apprécient plus il devient difficile pour les concurrents potentiels de rivaliser\n\nT’en pense quoi ?"
  },
  {
    "objectID": "blog/post_42.html",
    "href": "blog/post_42.html",
    "title": "Standard de qualité des données",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-data-datamanagement-activity-7073962169251291136-6ML_?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 12 Jun 2023\n\nUn élément CLÉ pour BIEN gérer la QUALITÉ DES DONNÉES….\nLe standard de qualité\nEn quoi ça consiste ?\nEt bien c’est l’ensemble des règles de qualité de données ainsi que les indicateurs associés.\nOn va écrire ces règles avec les producteurs et les consommateurs de données, en langage naturel.\nL’idée c’est qu’avec nos connaissances de la réalité…\nOnt définissent une liste de règles pour vérifier que les données en sont bien la représentation\nCe standard est spécifique à un jeux de données mais aussi à un groupe d’usage de la données en question\nSi les utilisations de la données sont très différentes, on peut imaginer que les besoins en termes de qualité le soit aussi\nDans ce cas, il vaut mieux avoir plusieurs jeux de données (et donc plusieurs standards) pour s’accommoder des différents besoins."
  },
  {
    "objectID": "blog/post_40.html",
    "href": "blog/post_40.html",
    "title": "Trust but verify",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-datamanagement-trustbutverify-activity-7071734965184393216-1Lmg?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 06 Jun 2023\n\nLa plupart des organisations sur-estiment le niveau de qualité de de leur données\nOn à tendance à vouloir croire en la véracité et l’exactitude des données\nDis autrement, les données sont vraies jusqu’à preuve du contraire\nDans une démarche de gestion de la qualité, on ne peut pas se permettre uniquement de croiser les doigts et d’espérer que les données sont bonnes…\nC’est notre job de vérifier !\nEt souvent quand on défini et qu’on mesure le niveau de qualité pour la 1ère fois\nIl y a souvent des surprises….\nJ’ai travaillé pour une société qui s’est rendu compte que pour deux de ses datasets clés, il manquait la moitié des relations\nOu une autre qui avait plus de 30% de doublons dans un référentiel fournisseurs\nAvant la mesure du niveau de qualité, personne dans l’organisation n’aurait imaginé que ce soit le cas…\nOn peut y voir une forme de similarité avec la sécurité informatique par exemple\nOn ne peut pas juste espérer que tous les utilisateurs/acteurs seront bienveillants…\nIl faut prévoir et tester les différents scénarios, et encore plus ceux qu’on ne souhaiterais jamais avoir à affronter"
  },
  {
    "objectID": "blog/post_39.html",
    "href": "blog/post_39.html",
    "title": "Dialogue fictif entre un CDO et un CEO…",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_au-fait-est-ce-que-tu-as-suffisamment-activity-7071372578048618496-iZM8?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 05 Jun 2023\n\n👨‍💼: “Au fait, est-ce que tu as suffisamment de budget pour la data quality”\n👨‍💼: “Tu es sûr ? N’hésite pas si tu as besoin de quoi que ce soit, on sait que c’est un sujet important, tout le Codir est derrière toi”\nJe pense que ces deux phrases n’ont probablement jamais été prononcées par aucun CEO…\nEt peut-être à juste titre…\nEntre professionnels de la données, l’importance et les bénéfices d’une bonne qualité paraissent évident\nMais pour le Codir, la qualité de données n’est que le sous-sujet d’un sous-sujet d’un sous-sujet,… au sein d’une problématique générale\nComment gérer au mieux l’organisation ?\nAvec toutes les considérations que ça impliquent\nAlors comment faire pour que des aspects, pourtant critiques comme la qualité de données, ne finissent pas par être négligés…\nJe pense qu’un des gros point faible de la data quality, c’est la difficulté de démontrer un ROI\nOn connaît tous l’astuce du business case qualitatif (dont les projets data sont en général très friand)\nMais avec le contexte économique actuel,\nAvoir la capacité de démontrer, de façon chiffré, qu’une amélioration de la qualité de données aura un impact bénéfique réel\nC’est peut-être une des clés pour permettre aux organisations de réaliser le potentiel de leur données."
  },
  {
    "objectID": "blog/post_37.html",
    "href": "blog/post_37.html",
    "title": "Six sigma et qualité de données, l’erreur classique",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-data-datagovernance-activity-7069560641505619968-MXos?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 31 May 2023\n\nL’ERREUR à ne PAS FAIRE si tu veux appliquer la méthode Six Sigma à la DATA QUALITY\nEst-ce que tu as déjà entendu parler de SPC (Statistical Process Control) ?\nPour résumer simplement, ça consiste à utiliser la statistique pour contrôler la stabilité d’un processus.\nÀ la base développées pour l’industrie, la plupart des méthodes de SPC reposent sur les principes suivant :\n• Dans un processus de fabrication stable, à entrée constante la variabilité en sortie va elle aussi être stable. • Tout écart trop important par rapport à la moyenne devrait donc être considéré comme une anomalie potentielle et être investigué.\nAvec l’émergence du paradigme “data-as-a-product”, on pourrait avoir envie de faire la même chose pour nos données ?\nEt bien OUI et NON…\nNON car beaucoup de gens font l’erreur de vouloir vérifier directement les données avec ces méthodes.\nExemple : Si une valeur du champ X de la table Y est à +/- 6 écart-types alors c’est une anomalie.\nLe problème c’est que nos données ne sont pas uniquement le produit d’un processus, mais aussi une représentation de la réalité.\nEt que l’on ne sait pas si notre échantillon (aka notre dataset) est représentatif de la population “réelle” (ce n’est peut-être pas un échantillon randomisé).\nNi qu’il contient des observations indépendantes entre elles.\nPour te donner un exemple, imaginons que tu veuilles mettre en place ce type de contrôle sur l’âge des salariés de l’organisation.\nTu fais tes calculs et tu te rends compte que l’âge moyen est de 32 ans, avec +/- 3 ans d’écart-type.\nTu fais tes z-scores pour tous tes employés et tous les employés de +50 ans vont être considérés comme des anomalies….\nCe qui n’est pas forcément le cas, c’est peut-être juste des employés plus âgés que la moyenne.\nEn fait il y a beaucoup d’autres raisons qui expliquent l’écart qu’une erreur de qualité de données.\nCe n’est donc pas une façon efficace pour vérifier la qualité des données.\nEn revanche OUI, il est possible d’utiliser ce genre de méthode pour la DATA QUALITY.\nMais je garde le suspense… et je te dirai comment exactement dans une de mes prochaines vidéos !"
  },
  {
    "objectID": "blog/post_35.html",
    "href": "blog/post_35.html",
    "title": "4 frameworks de gestion de la qualité de données à connaitre !",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_4-framework-de-data-quality-dont-tu-peux-activity-7067386330778882048-p3oD?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Thu, 25 May 2023\n\n4 framework de DATA QUALITY dont tu peux t’inspirer pour créer le TIEN !\n• Total Data Quality Management du MIT Information Quality Program\nConçu lors d’une activité de recherche pour définir une théorie de la data quality Propose la notion d’information product Les publication du MIT Information Quality (MITIQ) Program sur leur site web sont très intéressante même si plus màj\n• 10 steps Data Quality Framework de Danette McGilvray\nSûrement le plus connu, orienté projet “one-shot” Le modèle est pratique et efficace, également un des rare à aborder l’aspect communication\n• Total Information Data Quality Management by Larry English\nConçu initialement pour des projet de datawarehouse “Old but gold”, une des références en la matière Modèle de coûts/bénéfices limité en termes de données mais très intéressant (un des plus détaillé que j’ai vu jusqu’à présent)\nEn bonus, un autre que je connais pas bien mais que auxquels plusieurs ouvrages font référence :\n• Istat Data Quality Methodology from Italian National Institute of Statistics\nA la particularité d’avoir été conçu pour une administration publique avec plusieurs agences (centrales, locales, périphériques) Pour l’instant, je n’en sais pas grand-chose de plus, il faudra creuser par toi-même si ça t’intéresse\nSi tu connais d’autres framework de gestion de la qualité intéressant n’hésite pas à les partager en commentaires !"
  },
  {
    "objectID": "blog/post_33.html",
    "href": "blog/post_33.html",
    "title": "L’iceberg de la data quality",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-datamanegement-datagovernance-activity-7066661564455505920-2Wds?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 23 May 2023\n\nLes anomalies de la qualité de données viennent rarement seules…\nC’est en général le symptôme d’un problème plus profond avec le SI et/ou les processus métiers\nD’où l’importance de ne pas faire de la data quality en silo (champ d’action limité)\nEt d’associer la démarche qualité à une gouvernance\nSans quoi tu risque de courir après les anomalies…"
  },
  {
    "objectID": "blog/post_31.html",
    "href": "blog/post_31.html",
    "title": "L’importance de s’attaquer au causes racines de la non-qualité",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-data-datamanagement-activity-7061950506369179648-g9dM?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 10 May 2023\n\nEst-ce que tu connais le point commun entre un MARIN et un DATA QUALITY MANAGER ?\nNon… ? 🤔\nJe te propose de découvrir le point commun entre réparer un bateau qui coule et résoudre des problèmes de qualité de données ⬇\nPour une amélioration continue de la qualité, je pense qu’il est nécessaire d’avoir une approche radicale.\nDans le sens ou notre approche doit adresser les causes racines à l’origine des problèmes de qualité\nEt ne pas juste “rester en surface”\nPour illustrer ce point, j’aime bien l’analogie du bateau….\nOn imagine qu’on est sur bateau en mer et que la coque soit percée, le bateau commence donc à se remplir d’eau et à couler\nRésoudre en “firefighting” les anomalies et l’équivalent de prendre un seau et de vider l’eau\nC’est peut-être nécessaire dans certains cas afin d’éviter l’imminence du naufrage\nMais ça ne résoudra pas le problème sur le long terme\nUne approche plus pérenne consiste à chercher la fuite, à la boucher puis ensuite à vider l’eau du bateau\nEt pour la gestion de la qualité, l’approche est similaire\nIl faudra parfois corriger en urgence certaines anomalies….\nMais le gros du travail devrait être orienté vers la résolution des causes racines puis la correction des données"
  },
  {
    "objectID": "blog/post_3.html",
    "href": "blog/post_3.html",
    "title": "Data quality dimensions",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-dama-data-activity-7016309770659766273-1AiP?utm_source=share&utm_medium=member_desktop\nDate originale de publication :\n\nI have stopped using hashtag#dataquality dimensions and you should too ! Here’s why… ⬇\nCan you really tell the difference between accuracy, consistency and validity dimension without looking at the hashtag#dama DMBOK ? I find this notion a bit confusing and difficult to explain to business people.\nI often see the case where those dimensions are used as a way to define data quality indicators (i.e. : accuracy rate or consistency rate) and usually there is not much clarity on what those indicators mean. The main problem is that interpretation of those dimensions can vary drastically depending on context and the audience.\nIt is more useful to know what the metric represents exactly in plain business terms rather than know that it is the accuracy/consistency/validity/synchronicity/whatever dimensions indicator.\nData quality indicators must be factual, precise, driven by business rules and cannot not rely only on those notions as they are too much subject to interpretation. It must also be easily shareable with people that are not be knowledgeable on those concepts as addressing the root cause(s) might involved virtually anybody in the organization.\nI’m not saying we should dump data quality dimension but maybe take a step back. Under the assumption that data quality rules are in fact business rules, the data quality indicators should measure if we comply with our business rules. Then data quality dimensions can be used as a way of categorizing our business rules and subsequent data quality indicators but not as the main driver for rules definition.\nTo illustrate that, the reasonability rate of your customer referential most likely doesn’t ring any bell to your business stakeholders or if it does chances are that the exact meaning is unclear… at best. On the other hand if you talk about the rate of invoice not associated with a valid customer, the exact meaning is much more perceptible at a glance, which would contribute to a better understanding of data quality by business.\n\nOriginal picture from Andrea Piacquadio on Pexels"
  },
  {
    "objectID": "blog/post_27.html",
    "href": "blog/post_27.html",
    "title": "Créer de l’intérêt pour un projet data",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_la-technique-secr%C3%A8te-pour-cr%C3%A9er-de-lint%C3%A9r%C3%AAt-activity-7057601855534587904-T8Ga?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Fri, 28 Apr 2023\n\nLa technique secrète pour créer de l’intérêt pour ton projet data 😱 !\nEst-ce que tu t’es déjà retrouvé en galère de budget ou que tu n’arrive pas à créer de l’intérêt pour ton projet data ?\nC’est une situation que j’ai moi même rencontré comme beaucoup d’autre je pense 😁\nEt peu importe ton niveau de séniorité, tu vas sûrement devoir convaincre des gens de prêter attention à ce que tu veux faire\nDonc un conseil, sois bien attentif à ce poste !\nBon alors ce secret… .\nLa logique est très simple : Montre la douleur\nSi tu veux créer de l’intérêt à coup sûr, démontre comment ne PAS faire ce que tu propose est douloureux\nPlus douloureux que le statu quo\nOn est plus facilement motivé à éviter la douleur qu’à chercher du plaisir\nSurtout si le “plaisir” (comprendre les bénéfices de ton projet data) requiert des efforts…\nVoilà, tu connais la technique secrète !\nAttention cependant, cette technique est puissante\nTu risques de pas te faire que des amis au passage…\nÀ utiliser à tes risques et périls 😉\nEt surtout à coupler avec une bonne “intelligence de jeu”"
  },
  {
    "objectID": "blog/post_25.html",
    "href": "blog/post_25.html",
    "title": "Data flow map",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_architecture-dataflow-datamanagement-activity-7056877072584962048-I4VF?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 25 Apr 2023\n\nEst-ce que tu connais ce document SUPER UTILE pour s’y retrouver dans un SI COMPLEXE mais qui est souvent OUBLIÉ ? 😯\n…\nLa DATA FLOW MAP !\nEn français, cartographie des flux de données, est une représentation visuelle des échanges de données entre les applications d’une organisation.\nElle pour but de faciliter la compréhension des échanges de données entre les différents systèmes informatiques de l’entreprise\nNiveaux de grains (du moins fin au plus fin):\n\nApp-to-App et Process-by-process : Les flux de données entre applications sont représentés par processus métier\n\n(ex : CRM -&gt; ERP | Sales to Cash).\n\nApp-to-App et Data-by-Data : Les flux de données entre applications sont représentés données par données\n\n(ex : CRM → ERP | Référentiel client).\n\nApp-to-App et Function-by-Function : Les flux de données entre applications sont représentés par fonction\n\n(ex : Client CRM → ERP | Réplication journalière des nouveaux clients du CRM vers l’ERP).\nComment créer une cartographie des flux de données à partir de zéro ?\nNe vise pas la perfection du 1er coup, c’est impossible d’être exhaustif et précis dès le départ.\nCommence par ce qui est déjà connu et met en perspective les informations recueillies.\nPlan étape par étape :\n\nClasser par priorité toutes les applications gérées par la DSI.\nPréparer un questionnaire pour recueillir les infos dont tu as besoin (application source/cible, données échangées, fréquence, déclenchement, fonction, techno, etc)\nIdentifier les responsables des applications côté IT (et si possible, les responsables côté métier également).\nOrganiser des ateliers avec chaque responsable d’application pour recueillir des informations sur les flux de données existants et futurs de leur application.\nÀ partir des informations recueillies, tu crées les différents niveaux de cartographie des flux de données.\n\nL’objectif est d’obtenir rapidement une première version pour faciliter la suites des discussions sur l’architecture globale"
  },
  {
    "objectID": "blog/post_23.html",
    "href": "blog/post_23.html",
    "title": "C’est quoi la data gouvernance ?",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_datagovernance-data-activity-7054702754232979456-5ngU?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Thu, 20 Apr 2023\n\nEn vrai c’est quoi une data gouv’… 🤔 ?\nAprès avoir travaillé de près ou de loin sur la gouvernance des données de 4 sociétés différentes au cours des 7 dernières années…\nJe voudrais te partager ma définition “pratique” de la hashtag#datagovernance.\nEn réalité une gouvernance de données c’est :\n\nAvoir une définition claire pour nos données (savoir ce qu’elle représente ces données !)\nSupporter l’interprétation en documentant les règles métiers, les modèle de données, les relations, etc\nAvoir des rôles et responsabilités clair sur la gestion des données l’entreprise (ce qui veut dire à minima qu’il y une personne qui est responsable sur chacune des données du périmètre)\nPermettre l’échange et la collaboration autour des données de l’entreprise à travers les différents domaines via un “réseau hashtag#data”\nAvoir un cadre qui permet de maintenir et d’améliorer tous les points décrits au-dessus\n\nQu’est ce que tu en penses ?"
  },
  {
    "objectID": "blog/post_20.html",
    "href": "blog/post_20.html",
    "title": "Contrôle de qualité et problèmes de qualités de données",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-dataquality-qualitaezdedonnaezes-activity-7051803633595609088-qrnR?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 12 Apr 2023\n\nSi ton process de DATA QUALITY est centré principalement sur les CONTRÔLES DE QUALITE, tu as TOUT FAUX !\nCe qui importe VRAIMENT, ce sont les PROBLEME DE QUALITE des données\nLe nombre et la criticité des contrôles ne devraient pas être la chose sur laquelle tu te concentre le plus….\nEvidemment, tu devrais effectuer autant de contrôles que nécessaire, sinon autant que possible.\nDans la plupart des cas, les contrôles de data quality ne coûte pas grand-chose (ni en temps ni en euros)\nA moins que tes contrôle de qualité soient très sophistiqué\nOu que qu’ils soient complètement fait à la main dans un obscur fichier Excel….\nCe qui devrait être l’obsession de chaque data quality manager c’est les problèmes de qualité des données.\nLe nombre de problèmes et leur criticité sont beaucoup plus importants !\nCar c’est la résolution de ses problèmes qui va permettre une améliorations de la qualité, et non les contrôles eux-mêmes.\nEn fin de compte, les 2 métriques qui compte vraiment pour ton processus de gestion de la qualité des données (surtout au début) sont :\n\nLes problèmes de qualité identifiés\nLes problèmes de qualité résolus"
  },
  {
    "objectID": "blog/post_19.html",
    "href": "blog/post_19.html",
    "title": "Le pire ennemi du professionel de la data",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-doublons-datacleansing-activity-7051463824742207488-wuAz?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 11 Apr 2023\n\n\nTous les pros de la data m’ont déjà rencontrés au moins une fois dans leur carrière….\nJe viens fausser tes résultats d’analyse et créer des anomalies dans tes reporting\n\nQui suis-je… ?\nUn doublon !\nEt oui les doublons sont une des problématiques majeures de la qualité de données.\nAlors comment faire pour ne pas en avoir (ou du moins en avoir le moins possible) ?\n\nIdentification\n\nIdentifier des doublons c’est plus un art qu’une science !\nLa première étape d’un dédoublonnage est de “computer” toutes les paires possibles (produit cartésien du jeu de données). Puis il faut déterminer si chaque paire est un doublon\nC’est là ou ça commence à se corser…\nSelon le niveau d’exigences, le modèle de données et les règles métiers, la façon d’identifier des doublons ne sera pas la même.\nPour un modèle de données simples, une simple comparaison exacte sur un ou plusieurs attributs peut suffire\nEn revanche pour des données plus complexes, il faudra surement utiliser des contrôles plus avancés\n2 exemples connu :\n\nLa distance de Levenshtein, qui mesure la similarité entre deux chaînes de caractères\nLe Soundex qui permet de comparer la similarité de prononciation entre deux chaînes de caractères\n\nOn peut même combiner les différentes méthodes et avoir un score de similarité pour chacune.\nIl nous faut ensuite séparer les paires en doublons de celles qui n’en sont pas\nPour ça plusieurs méthode peuvent-être utilisé, allant de la plus simple (”au doigt mouillé”) à des méthodes de classification plus complexes (random forest, apprentissage actif, etc)\n\nCorriger\n\nLa prochaine étape est le regroupement de nos doublons entre eux avant de choisir l’entrées la plus qualitative.\nLa encore plusieurs méthodes peuvent être utiliser pour le groupage, à définir selon les exigences en termes de qualité\nLe plus simple étant une approche naïve —&gt; Si A et B son doublon et B et C sont doublons alors A et C sont doublons\nOn veut évidemment garder l’entrée qui à la meilleure qualité. Pareil plusieurs manières de faire, la plus simple étant de garder l’entrée avec le plus d’attributs remplis\nUne bonne pratique est de garder un historique complet de l’opération pour pouvoir revenir en arrière en cas de problème.\nIl faut aussi penser à gérer le transfert des autres jeux de données rattachés vers notre jeux de données dédoublonnées !\n\nPrévenir\n\nUtiliser un identifiant unique ! Même si il ne prévient pas directement des doublons, il permet d’identifier chaque entrée de façon équivoque et constitue donc un préalable au dédoublonnage\nL’implémentation de contrôle de de type “matching” (qui peut être manuelle ou automatisée) pendant ou juste avant la saisie afin de nous assurons que le processus de saisie des données ne génère pas de doublons."
  },
  {
    "objectID": "blog/post_17.html",
    "href": "blog/post_17.html",
    "title": "Problème de qualité de données et désastre opérationel à la NASA",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_saviez-vous-que-la-nasa-%C3%A0-perdu-125m-%C3%A0-cause-activity-7048649012710633473-UQ7G?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 03 Apr 2023\n\nSaviez-vous que la NASA à PERDU $125M à cause d’une MAUVAISE DATA QUALITY ?\nLa data quality n’est un sujet “sexy” pour tout le monde et peut parfois sembler loin de la réalité concrète.\nJe vais donc vous raconter, comment la NASA à perdu $125M à cause d’une mauvaise qualité de données\nLe 11 décembre 1998, une sonde spatiale la NASA lance une sonde spatiale en direction de la planète Mars afin d’étudier l’atmosphère et le climat de Mars\nLe coût total de cette sonde à été évalué à $125M\nL’objectif étant de ramener des mesures et des données scientifiques sur l’atmosphère et le climat de la planète.\nMais en Septembre 1999 au moment où la sonde devait entrer en orbite avec la planète, ils perdent tout contact avec l’appareil.\nEn fait l’équipe de navigation du Jet Propulsion Laboratory (JPL) a utilisé le système métrique pour tous ses calculs.\nAlors que Lockheed Martin Astronautics à Denver, dans le Colorado, qui a conçu et construit l’engin spatial, a fourni des données d’accélération en système anglais.\nLes ingénieurs du Jet Propulsion Lab de la NASA ont supposé que la conversion avait été effectuée, car c’était une pratique courante, mais ce n’était pas le cas.\nCela a entraîné des erreurs de trajectoire pour la sonde spatiale, l’envoyant trop près de l’atmosphère de la planète et provoquant sa destruction.\nVoilà comment une simple erreur d’unité de mesure peut conduire à un vrai désastre et à une perte financière conséquente."
  },
  {
    "objectID": "blog/post_14.html",
    "href": "blog/post_14.html",
    "title": "Data in PR vs Data in Reality",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_datamanagement-dataquality-digitaltransformation-activity-7041700222892167169-P0JI?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 15 Mar 2023\n\nData Management: PR Talk vs. Reality\nEver noticed how sometimes companies boast about their data-driven strategies in public relations, but behind the scenes, their data management practices tell a different story?\nIn PR: “Thanks to our successful digital transformation, we are a data-driven company leveraging AI to innovate and grow.”\nIn Reality: “We don’t need data quality management, our data is already good. And why do we need to document our data?”\nI believe the issue stems from a lack of clarity. Being data-driven or achieving digital transformation is not an end in itself; it’s merely a means (for all my marketing folks out there: it’s a feature, not a benefit).\nFor me, the real questions should be:\n\nWhat does your company want to accomplish?\nHow can data help move towards those goals?\n\nFrom there start small and iterate\nBut if you chase buzzwords, that’s what you’ll get in the end… buzzwords."
  },
  {
    "objectID": "blog/post_12.html",
    "href": "blog/post_12.html",
    "title": "Data VS Information",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-information-debate-activity-7037779968621260800-pVY7?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 01 Mar 2023\n\nMost data pros agree on the fact that data is not the same as information.\nBut are they really that different ?\nLet’s take a look at commonly used definitions for both concepts :\n\nData: Raw, unorganized facts that need to be processed.\nInformation: When data is processed, organized, structured or presented in a given context to make it useful.\n\nAt first sight those definitions that take roots from the DIKW pyramid, sounds OK but i think there is a caveat…\nWhen most people, even us data pros, talk about data, 90% of the time, we talk about structured data\n(the percentage is a wild guess but you get my point)\nAnd if it’s structured it means it must have some context.\nThink of a spreadsheet table, you would have the colum name and row number for each “data”\nSo isn’t it an information then…. ?\nThat would mean that only unstructured data are real data and structured data are information ?\nAlso, can information originate from something other than data ?\nOutside of the industry, people don’t know the difference and just want useful data/information.\nWho would care about random figures, without any context, that are unorganized and meaningless, right?\nSo are they really different ? Does it matter that much ?\n\nAn interesting exchange in the comment :\n\n“My thoughts on information derived from data come from questions asked of the cleaning process. E.g. how do we still keep inputting bad data or data out of limits, where is this data from, who logged it? I think information from other sources is possible, maybe thought experiments i.e., set up the experiment prior to obtaining data, informs the strategy for the collection process. Also, no data in itself informs information.”\n“From the cleaning angle it make more sense to me. I would go for something like information help reducing uncertainty/making decision and data can be or not be an information depending on the question/decision”"
  },
  {
    "objectID": "blog/post_10.html",
    "href": "blog/post_10.html",
    "title": "What NOT to do for improving data quality",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-datamanagement-digital-activity-7034069227783249920-JuHr?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 22 Feb 2023\n\nHey, it is well-known that data quality is not a big deal, right… ?\nSo let’s look at 6 tips together to make sure it doesn’t improve :\n\nTreat data quality as a technical problem only\n\n\nUse only technical solutions to improve it (e.g., software, probabilistic controls, etc.).\nDo not involve business SMEs (data producers, data consumers) to ensure that data quality management is as far from business priorities as possible and that they do not understand it.\n\n\nDo not use common and clear guidelines for data quality management\n\n\nStress everyone in the company about bad data quality and that it’s their job to improve it.\nDo not define any systematic process for data quality management or any guidelines.\n\n\nFocus only on theoritical aspects\n\n\nMake sure to have all data quality dimensions quoted in the literature (60+) covered by your data quality process.\nMake your process documentation highly complicated and full of jargon. This way, nobody will be able to contribute to it or understand the meanings and methodology of indicators.\n\n\nPerform only data corrections, root cause don’t matters that much\n\n\nFocus all effort on data correction and firefighting quality issues where the “symptoms” occur.\nForget about root cause analysis and a holistic view of why we have an issue. It’s completely unnecessary.\n\n\nTreat all quality issue as the same no matter the business impacts\n\n\nThe real business impact of quality issues is irrelevant; make sure not to prioritize the high impact ones.\nIn fact, to save time, you can just not assess the impact of quality issues and use instead the “wet-finger” prioritization technique\n\n\nLaunch isolated data quality projects\n\n\nBy avoiding a common and continuous approach, you ensure information loss and prevent follow-through of the issues previously identified.\nIt also helps to create or reinforce existing silos in the company, which is always a good thing, especially when it comes to data."
  },
  {
    "objectID": "blog/post_0.html",
    "href": "blog/post_0.html",
    "title": "Data management buzzword",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-datacatalog-datagoverance-activity-7008705261292896257-qcX-?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 14 Dec 2022\n\nWe all have heard about hashtag#dataquality, hashtag#datacatalog or hashtag#datagoverance. Are they hashtag#buzzwords for you ? Or do you know clearly what they mean in your specific company context ?\nThe n°1 step of any data-related initiatives is to have clarity on what we actually want to accomplish. Just saying we are going to create a data dictionary or we are going to do data quality is not enough.\nAny data related terms can be understood differently depending on the audience. For instance the term data quality can refer to the level of data quality of a given data object, the complete process of managing data quality or a “one-shot” data quality investigation. This might create confusion if the term itself and the expected outcomes associated with it are not well defined.\nAs an example, one of my customer wanted to implement a data catalog solution though their maturity on hashtag#datamanagement and hashtag#datagovernance was very low (they didn’t had any type of governance in place at that time).\nDue to no governance and no business data owner involvement, they wanted to focus the data catalog on documenting only the technical aspects (database(s), table(s), SQL queries, etc)\nPurpose of a data catalog is to document company data so that any collaborator can easily know what it means, how to interpret it, where he can access it and who is responsible for it.\nThe focus should be on documenting business terms, ownership, business rules, policies, etc by starting from a “top-down” approach (top being business/natural language, bottom technical). Only once the business aspects are mastered, then the technical information becomes useful as they can be placed into a business context.\nMain takeaway here is that if you lack hashtag#clarity for your data project(s), you should work this out first !\n\nOriginal picture from Michelle Tresemer - Unsplash"
  },
  {
    "objectID": "blog/post_1.html",
    "href": "blog/post_1.html",
    "title": "Data quality rules",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-dataquality-datamanagement-activity-7010884645273673728-YOrQ?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 20 Dec 2022\n\nDo you struggle with data quality rules ? Let me share with you how i deal with definition of data quality rules :\nFirst, let’s think about hashtag#data as a digital representation of a “thing”. In a company context those “things” are usually objects that are being manipulated during the business processes of the company (i.e : Customer, Location, Material, Invoices, etc).\nThose “things” have different characteristics being represented by attributes (if we stay with Customer example the attributes could be First Name, Last Name, Address, etc).\nA high hashtag#dataquality means the business data object(s), stored in our information systems are :\n\n✅ An accurate representation of those “things”\n✅ Fit for the usage that we make of it as part of the business processes.\n\nTo improve data quality, we need to define what are the expectations for the characteristics of those business objects for them to be as such.\nSimply put, it means that we need to define the business rules of our objects and to measure through data quality indicators if our business objects are indeed compliant with those rules or not (on paper it sounds trivial doesn’t it ? 😁)\nLet’s make an example with the Invoice business objects of a retail clothing company :\nBy definition an invoice should have some mandatory characteristics (Issuer Name, Goods delivered and prices, VAT Number, etc), if an invoice doesn’t have one of those, it’s not a valid invoice ➡ Business Rules n°1\nAn invoice should be always associated to a Customer ➡ Business Rules n°2\nThe list goes on ….\nBased on those business rules, you define your DQ indicators (rate of incomplete invoice, rate of invoice not associated to a customer). Collection of all those rules with associated indicators is what i call a data quality standards and should be what’s monitored on regular basis by your data quality management process.\n\nOriginal picture from Ben White on Unsplash"
  },
  {
    "objectID": "blog/post_11.html",
    "href": "blog/post_11.html",
    "title": "How to explain the value of data to your colleagues",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-analytics-ai-activity-7036735231785132032-JzHv?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 01 Mar 2023\n\nDo you struggle to explain the value of data to your boss and co-workers? 🤔\nAs data professionals, we can get caught up in our own expertise and forget about the perspective of “non-data” folks.\nTo help bridge the gap, I’ve created a visual to show how data can tie into business outcomes.\nThere are three significant ways that data can impact a company (assuming you’re not a tech company):\n\nBetter decision-making (analytics): The ability to extract trustworthy information on your market and operations for business decisions.\nAutomation (AI): Innovation and operational improvement through automation for more value with less effort.\nAvoiding operational disasters (data quality): Bad data quality can cause disasters like NASA’s lost Mars Orbiter worth €125M due to a unit of measure error ! 😱\n\n(The NASA disaster will be the topic of an upcoming post, so make sure to follow me if you don’t want to miss it 😉)\nAll of these require the right peoples, tools, and processes to manage data end-to-end in your company.\nEasy peasy, right? 😇"
  },
  {
    "objectID": "blog/post_13.html",
    "href": "blog/post_13.html",
    "title": "Data management jargon",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-data-learning-activity-7039189646546718721-UK-H?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 08 Mar 2023\n\nDo you ever feel like you’re struggling to explain the data jargon to your coworkers? 🤯\nTrust me, I know what you feel!\nSo let me share with you, the definition I use to explain data quality to a non-data audience:\nFirst, you can think of data as a digital twin of a real thing.\nAs an example, customer data in your CRM are the digital twins of your “real” customers.\nData quality is about how accurate that digital twin is compared to the real thing….\nBut here’s the thing: quality is a subjective concept.\nLet’s take a pair of shoes… What makes them high-quality?\nIt depends on who’s wearing them and what they’re wearing them for, right?\nWell, it’s the same with data! Quality will depend a lot on the usage.\nTo keep with the shoe analogy, managing data quality is a lot like managing quality in a shoe factory. It requires to :\n\nDefine quality requirements\nRegularly check that the data meets your expectations\nAddress any complaints from consumers\nInvestigate the causes of quality issues.\n\nIt’s not always fun, but it’s definitely necessary !\nAre your data as good as stilettos on the rugby field or do you have your data cleats on? 😂"
  },
  {
    "objectID": "blog/post_15.html",
    "href": "blog/post_15.html",
    "title": "Persuade your boss/team about the value of your projects",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_est-ce-que-tu-gal%C3%A8res-%C3%A0-convaincre-ton-boss-activity-7044260184516362241-CePx?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 22 Mar 2023\n\nHaving trouble persuading your boss and team about the value of your data projects?\nI believe it’s a situation most of data pros have experienced at some point.\nIn such scenarios, my tactic is to clearly show how data can influence your boss’s objectives and your organization’s aspirations.\nIt’s great to have a quantifiable business case (my project will generate X€ in revenue / cut X€ in costs), but it isn’t always straightforward to measure.\nEspecially for topics such as data governance, data quality management or data catalog\nAs an alternative, consider the qualitative side; I enjoy using simple visuals like cause-and-effect charts.\nBy itself it may not secure a €1M budget, but if you can clearly establish the connection between data and management’s goals,\nIt will make everything else easier!\nHowever, if the purpose remains unclear, progress will be considerably more challenging.\nAs data professionals, I think it’s our responsibility to effectively convey the added value of our efforts to our colleagues.\nDo you employ other strategies for promoting data internally? Share your thoughts in the comments!"
  },
  {
    "objectID": "blog/post_18.html",
    "href": "blog/post_18.html",
    "title": "La data gouvernance comme les antibiotiques, c’est pas automatique !",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_datagovernance-data-digital-activity-7051111845520109568-MlCZ?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 10 Apr 2023\n\nLa gouvernance des données, c’est comme les antibiotiques… ce n’est pas automatique !\nAs-tu déjà passé des heures à mettre en place une gouvernance des données et à organiser d’interminables “data governance council”…\nPour finalement te rendre compte que peu de progrès concrets sont réalisés ?\nLa clé de la mise en place d’une gouvernance des données, c’est la clarté.\nSans but clair, la gouvernance des données devient simplement une autre case à cocher.\nEt il est peu probable qu’elle apporte une plus-value significative à ton organisation et risque plus d’être une perte de temps et d’énergie.\nOK, mais alors, c’est quoi la solution ?\nMon approche consiste à considérer la gouvernance des données comme un réseau !\nUn réseau sur lequel on va s’appuyer pour prendre des décisions, transmettre et récupérer des informations et faire exécuter nos différentes initiatives dans les différents domaines (Finance, Marketing, RH, etc).\nLe plus important est donc d’identifier les bons acteurs et d’animer ce réseau afin d’entretenir une dynamique collaborative et saine.\nLe processus et le cadre sont évidemment à prendre en compte, mais ils passent au second plan par rapport au groupe et aux individus.\nPour résumer simplement, je dirais que :\nBon processus de gouvernance + mauvaise dynamique de groupe = mauvaise gouvernance\nProcessus de gouvernance moyen + bonne dynamique de groupe = bonne gouvernance"
  },
  {
    "objectID": "blog/post_2.html",
    "href": "blog/post_2.html",
    "title": "Data quality target level",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-businessrules-data-activity-7013428267151970304-iMC-?utm_source=share&utm_medium=member_desktop\nDate originale de publication :\n\n🗣 : “How do you define data quality target levels ?” 🗣 : “Well… you don’t !”\nI do not recommend to set goals in terms of hashtag#dataquality levels, unless you are already advanced in your data quality management process.\nThe main 3 reasons why i don’t recommend setting goals are ⬇\n\n⛔ Not enough business clarity on your data quality indicators. You probably don’t have enough understanding yet on the hashtag#businessrules that govern your data objects, so your data quality indicators might not be that significant in terms of quality (accurate representation and fit-for-purpose in terms of business processes)\n⛔ No full picture on all your current data quality issues and their business impact (due to the point above), so no precise idea of their relative priority.\n⛔ No complete understanding of the root causes of those issues. If this the case, the effort required to resolve the issues remains unknown, neither if it’s actually possible or a good trade in terms of cost/benefits.\n\nIf all those aspects are covered within systematic, iterative and “evidence-based” data quality management process then only i would start setting goals in terms of what level of data quality for a given dataset/data object i want to achieve over a period.\nIn my opinion, a better way to measure progress in the beginning of your data quality journey would be the coverage of the data quality management in terms of business scope and/or percentage of data quality issues resolved over a period.\nIn a way it’s like dieting, when you start you do not have direct control over the final outcomes (how much weight you lose/gain) but you have control over the process (sticking to it over time) and that is where the improvement comes. After a while, once you know your body well and how you react to diet, then you can set more realistic goals in terms of weight gain/loss over the next months.\nWhat do you think about this post ? If you liked it feel free to like it and share it ! If you have any comments, questions or suggestions i would really like to have your opinion in the comments\n\nOriginal picture from Andrea Piacquadio on Pexels"
  },
  {
    "objectID": "blog/post_21.html",
    "href": "blog/post_21.html",
    "title": "Dimensions de la qualités de données",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-data-qualitaezdedonnaezes-activity-7052528403819683841-S04l?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Fri, 14 Apr 2023\n\nLes DIMENSIONS de la DATA QUALITY sont-elles réellement UTILES ?\nJe veux dire…, est-ce tu peux honnêtement me dire la différence entre la précision, la cohérence et la conformité sans regarder dans le DAMA ?\nSi une donnée est cohérente… c’est qu’elle conforme à une règle non ?\nEt si elle est précise c’est qu’elle est conforme avec la réalité ? 🤨\nJe ne dit pas que les dimensions sont complètement inutile mais plutôt que ce n’est pas le seul paradigme pour définir des indicateurs de qualité\nElles sont souvent même une source de confusion (selon où on regarde, il y a entre 5 et 60 dimensions… 😱)\nJe préfère penser les règles de qualité de données comme le reflet des règles métiers, tes indicateurs de qualité de données devraient donc mesurer si tes données respectent les règles métiers.\nEn revanche si tu te base uniquement sur les dimensions pour définir des indicateurs de qualité (ex : Taux de précision ou taux de cohérence) tu risque de créer de la confusion sur la signification de l’indicateur.\nLe souci étant que l’interprétation de ces dimensions va grandement dépendre de ton audience, du contexte, etc\nJe préfère savoir et communiquer ce que l’indicateur mesure en termes simples, plutôt que de savoir qu’il se rapporte à la dimensions X ou Y….\nLes indicateurs de qualités de données doivent être factuel, précis et basé sur des règles métiers et ne peuvent donc pas s’appuyer uniquement sur les dimensions de la qualité de données\nEgalement, si tu peux rendre tes indicateurs compréhensibles par les gens qui n’ont pas lu le DAMA ça ne peut-être qu’un plus pour aider à la diffusion et la compréhension de la démarche qualité à tes collègues “non-data”\nPour illustrer mon propos, je te propose cet exemple :\n\nLe taux de raisonnabilité du référentiel Client ne parlera probablement à personne\nLe taux de facture non-associées à un client valide, en revanche est beaucoup plus compréhensible, même pour un non-initié"
  },
  {
    "objectID": "blog/post_24.html",
    "href": "blog/post_24.html",
    "title": "Stratégie data : Objectifs vs Moyens",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-strategy-activity-7056514688972390400-v0D2?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 25 Apr 2023\n\nTu sais c’est quoi le point COMMUN de la plupart des MAUVAISES stratégie data ?\nElles sont orientées principalement sur la technologie….\nUne bonne stratégie data devrait être fixée sur les priorités et les enjeux de la société\nEn se basant sur des cas d’usage concrets et si possible quantifiable\nEt pas sur la dernière techno ou concept à la mode\nCe serait comme dire à quelqu’un dont l’objectif est de se remettre en forme, de construire une salle de sport dans son garage\nC’est pas inutile en soi mais c’est pas ça qui va la rapprocher le plus de son objectif\nIl faut évidemment mettre ça en perspective et ne pas tomber dans un biais uniquement court terme\nMais en se basant sur la finalité réelle, pas les moyens employés"
  },
  {
    "objectID": "blog/post_26.html",
    "href": "blog/post_26.html",
    "title": "C’est quoi un data manager ?",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-datamanagement-datamanager-activity-7057239484794302464-PhiJ?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Thu, 27 Apr 2023\n\nAu fait, c’est quoi un “data manager” ? 🤔\nLaisse-moi te présenter ce rôle, un peu moins connu du grand public mais qui est pourtant présent dans toutes la plupart des entreprises avec une stratégie data !\nParfois appelé aussi data governance manager ou data quality manager, le data manager est responsable de la gestion des données de l’entreprise.\n“Et c’est quoi la gestion des données de l’entreprise ?”\nDoucement j’y viens….\nLe but de la gestion de données c’est de faire en sorte que l’entreprise tire le meilleur parti de ses données en s’assurant qu’elles soit :\n\nde bonne qualité\naccessible aux bonne personnes\nau bons endroits\net utilisées de manière optimales\n\nPour que ce soit plus concret voilà une liste de tâches qu’un data manager peut-être amener à faire :\n\nDéfinir le processus de création/modification/archivage d’une données dans un référentiel\nEffectuer un audit de la qualité de données\nMettre en place une démarche d’amélioration continue de la qualité des données\nDéfinir les rôles et responsabilité autour des données (qui est responsable de quoi pour de quelle données)\nAnimer un réseau de référent data au sein des différents domaines métiers\nDocumenter le modèle de données conceptuel\nDocumenter les définitions, règles métiers, etc associées au données\n\nC’est un acteur clé de la stratégie data d’une entreprise car il va faciliter et débloquer la création de valeur par les autres acteurs de la chaîne (data engineer, data analyst, data scientist, etc)\nSans data manager, les tâches qui lui sont normalement affectées se retrouvent dispersées sur les autres acteurs qui doivent les gérer en plus de leur job…\nPas top comme situation !"
  },
  {
    "objectID": "blog/post_29.html",
    "href": "blog/post_29.html",
    "title": "L’importance de la clarté dans les projets data",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-gestiondeprojet-datamanagement-activity-7059413811090345984-P3PD?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 03 May 2023\n\nPlusieurs MOIS et des dizaines de K€ dépensé pour RIEN ! 😱\nJe vais te raconter une histoire VRAIE qui m’est arrivée sur un PROJET DATA qui n’aurait jamais dû être lancé….\nJe travaillais sur la gouvernance et la qualité des données pour une grosse société\nEt avant même que je prenne mon poste, un projet de création d’un référentiel pour une nouvelle donnée avait été lancé.\nL’équipe projet avait passée environ 1 an à essayer de naviguer entre les conflits et la politique interne de l’entreprise (la données en question étant un sujet sensible) sans réel progrès\nOn avait même pas de définition claire de pour cette nouvelle donnée\nAutant te dire que le référentiel n’était toujours pas prêt de voir le jour….\nPour débloquer la situation, un atelier à donc été organisé par le chef de projet lors duquel les questions suivantes ont été posées :\n• Pourquoi est-ce qu’on fait ce nouveau référentiel en fait ?\n• C’est quoi cette nouvelle donnée ?\nIl s’est avéré que cette “nouvelle” donnée était en réalité quasi-identique à une autre déjà existante dans un autre référentiel…. 😅\nEt que ce projet de nouveau référentiel avait été lancé uniquement dans le but de répondre à une question posée par un top exec’ il y a de ça plus d’1 an….\nAprès quelques autres réunions et en impliquant les bonnes personnes, le problème était quasi-résolu.\nEn utilisant les données déjà existantes on pouvait répondre à 95% de la question posée…\nSans projet, sans budget, sans une armée d’externes, et sans délai….\nVoilà pourquoi je pense qu’il est important d’avoir de la clarté sur POURQUOI on fait ce que l’on fait\nAutrement on peut vite se perdre dans le COMMENT, perdre de vue la finalité et finir par faire fausse route"
  },
  {
    "objectID": "blog/post_30.html",
    "href": "blog/post_30.html",
    "title": "Contrôle de qualité de données déterministe vs probabiliste",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_contr%C3%B4le-de-qualit%C3%A9-de-donn%C3%A9es-d%C3%A9terministe-activity-7060138562465280000-QDzQ?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Fri, 05 May 2023\n\nContrôle de qualité de données DÉTERMINISTE vs PROBABILISTE…\nEst-ce que tu connais les différences entre ces deux types de contrôle ?\nJe te propose cette petite infographie qui résume mes connaissances sur le sujet ⬇️\nContrôle déterministe (basé sur des règles et une logique conditionnelle) :\n\nQuestion : Est-ce que cette donnée est une anomalie ?\nAvantages : Facile à interpréter\nInconvénients : Les règles doivent être définis à la main et nécessitent une certaine connaissance du domaine métier\n\nContrôle probabiliste (détection d’anomalies, etc) :\n\nQuestion : Est-ce qu’il est probable que cette données soit une anomalie ?\nAvantages : Ne nécessite pas des règles formelles\nInconvénients : Interprétabilité plus délicate (faux positif/faux négatif), théorie du monde clos et dépendance au niveau de qualité initiale\n\nÉvidemment les deux approches ne sont pas opposées et peuvent se compléter\nMais dans un souci de simplicité, je préfère me concentrer en premier lieu sur les contrôles déterministe\nQui me permettent de transitionner plus facilement vers les autres étapes de la démarche qualité\nEn revanche, si je suis sur une démarche qualité déjà rodée et que je veux aller un peu plus loin, dans ce cas les contrôles probabilistes sont très intéressants (notamment sur du dédoublonnage)."
  },
  {
    "objectID": "blog/post_32.html",
    "href": "blog/post_32.html",
    "title": "4 livres à lire sur la gestion de la qualité de données",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_4-ouvrages-%C3%A0-lire-absolument-si-tu-tint%C3%A9resse-activity-7065211992893972480-GEF4?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Fri, 19 May 2023\n\n4 ouvrages à LIRE ABSOLUMENT si tu t’intéresse à la DATA QUALITY :\n• Danette McGilvray - Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information (2008)\nUn classique, le livre est très complet aborde la qualité de données sous un angle pratique. Parfait si tu veut comprendre et mettre rapidement en place un projet d’amélioration de la qualité de données\n• Jack E. Olson - Data Quality: The Accuracy Dimension (2003)\nTrès bon livre, très complet aussi. J’ai trouvé le chapitre sur le business case très pertinent, on sent que l’auteur à une grosse expérience du sujet.\n• Lowell Fryman, Gregory Lampshire, Dan Meers: The Data and Analytics Playbook: Proven Methods for Governed Data and Analytic Quality (2016)\nUn peu plus orienté data governance, conçu comme “un playbook” pour un leader data pourrait suivr, le livre décrit de façon intéressante certaine des situations “politiques” autour de la governance des données\n• Laura Sebastian-Coleman - Meeting the Challenges of Data Quality Management (2022)\nPlus récent que les précèdents, l’ouvrage est encore plus complet puisqu’il aborde aussi d’autres thématiques comme la stratégie ou la culture data."
  },
  {
    "objectID": "blog/post_34.html",
    "href": "blog/post_34.html",
    "title": "3 définitions pour mieux gérer ses données d’entreprises",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-datamanagement-datagovernance-activity-7067023930263248896-Q3tB?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 24 May 2023\n\n3 définitions SIMPLES pour POSER LES BASES quand tu parle DATA\nC’est pas toujours facile de parler data\nEntre le data mesh, la data fabric, la data observability ou encore la data gouvernance….\nOn peut vite s’y mélanger les pinceaux, même pour les pros !\nImagine quelqu’un qui n’est pas du domaine…\nAlors pour partir sur de bonnes base quand tu parle data avec quelqu’un qui n’est pas familier avec la terminologie\nJe te propose ces trois définitions :\n• Données : Représentation digitale (çàd dans un SI) d’un objet ou d’un événement réel\n• Attribut : Représentation digitale d’une caractéristiques d’intérêt de notre objet/évènement\n• Modèle de données : Ensemble des attributs d’un objet/événement"
  },
  {
    "objectID": "blog/post_36.html",
    "href": "blog/post_36.html",
    "title": "4 frameworks de gestion de la qualité de données à connaitre !",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-datamanagement-data-activity-7069198262502617088-IW8t?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 30 May 2023\n\n3 causes possibles de doublons dans un jeu de données :\n• Intégration de plusieurs sources sans réconciliation :\nSi l’on intègre des référentiels clients provenant de différentes sources dans un entrepôt de données sans les réconcilier entre eux, il est fort probable que des doublons se forment.\n• Pas de vérification lors de la création :\nL’étape de création/saisie des données est cruciale. Un contrôle lors de la saisie pour prévenir la création de doublons dans le référentiel peut éviter de nombreuses anomalies !\n• Pas d’identifiant unique :\nIl est surprenant que cela existe encore en 2023… Cependant, il est possible que certains systèmes ou processus ne génèrent pas d’identifiant unique pour les différents objets/événements utilisés, ou qu’ils les gèrent mal, ce qui contribue aux causes possibles de duplication."
  },
  {
    "objectID": "blog/post_38.html",
    "href": "blog/post_38.html",
    "title": "3 méthodes pour calculer un indice de qualité des données",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-datagovernance-data-activity-7070285435175858176-2WwW?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Fri, 02 Jun 2023\n\n3 METHODES pour calculer un INDICE de DATA QUALITY !\nSi tu t’intéresses à la data quality, tu vas peut-être vouloir définir un indice de qualité…\nC’est une bonne façon d’agréger les différents indicateurs de qualité pour avoir une idée de la qualité globale\nJe vais te partager 3 méthodes que j’utilise pour définir et calculer un indice de data quality\nOn va partir du postulat que tes indicateurs de qualité sont exprimés en % du nombre d’entrées totales d’un jeu de données\n• Moyenne des indicateurs (equi-pondération)\nFacile, tu fais la somme des indicateurs de qualité et tu divises par le nombre d’indicateurs de qualité\nProblème : Toutes les anomalies ne se valent pas\n• Moyenne pondérée des indicateurs\nCette fois on va assigner un poids à chaque indicateur pour pondérer le résultat, permettant de refléter son importance relative dans l’indice\nProblème : Définition des poids subjective\n• % d’entrées sans anomalies\nOn va simplement faire le % des entrées qui passent tous les contrôles.\nC’est personnellement ma préférée car elle reflète le mieux ma vision de la qualité de données.\nSi on part du principe que le standard de qualité = définition de qualité, cette méthode nous donne le % le plus représentatif.\nL’inconvénient c’est qu’elle reste aveugle par rapport à l’importance relative des différents contrôles…"
  },
  {
    "objectID": "blog/post_4.html",
    "href": "blog/post_4.html",
    "title": "Data quality management process",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-quality-management-activity-7018491937175306241-AtNb?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 10 Jan 2023\n\nLet me explain the process I use with my clients to manage their data quality 👇\n📜 Define initial standards for monitoring hashtag#dataquality\n\nDocument business rules and derive associated data quality indicators\nDefine monitoring methodology (frequency, queries used, etc.)\n\n👀 Start monitoring\n\nConduct initial data quality monitoring based on the newly defined standards\nIdentify current data quality issues (non-compliance with one or more business rules)\n\n🧠 Investigate root causes and effort for data correction\n\nAnalyze the data quality issues to understand what the root cause(s) are and what effort is required to correct the data affected by them\n\n🗣️ Communicate findings with the business and establish priorities\n\nShare your findings with business stakeholders and determine the criticality of the issue based on business impact\nDetermine priority based on cost (effort required to correct the data and root cause) and benefit (business impact)\n\n🔧 Correct data and root cause(s)\n\nIdeally, data correction should be done in the source system\nDepending on the context (legacy application that cannot be updated, etc.), sometimes the correction needs to be done on the fly between the source and the data warehouse\n\n🔄 Iterate\n\nEnsure that data consumers have the opportunity to raise issues not covered by the process\nThe quality standard should be updated after new business/quality rules are created or discovered"
  },
  {
    "objectID": "blog/post_41.html",
    "href": "blog/post_41.html",
    "title": "Valeur financière d’une données",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_datavaluation-infonomics-data-activity-7072459755050422273-5P-Q?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Thu, 08 Jun 2023\n\nC’est POSSIBLE de calculer la VALEUR FINANCIÈRE d’une donnée ? 💰 🤔\nC’est ce que Douglas Laney propose dans son livre “Infonomics”\nNotamment avec cette notion qu’il appelle l’EVI (Economic Value of Information)\nC’est une valeur sert à mesurer à quelle point cette données contribue à la profitabilité d’une business unit\nLa formule ⬇️\nEVI = (Chiffre d’affaires A - Chiffre d’affaires B - Coûts de gestion) * T/t\nChiffre d’affaires A = Chiffre d’affaires réalisé en utilisant l’information Chiffre d’affaires B = Chiffres d’affaires réalisé sans l’information (groupe contrôle)\nCoûts de gestion = L’ensemble des coûts d’acquisition, de stockage et maintenance de la données\nT = Durée de vie moyenne d’une instance de l’information t = Durée de l’expérimentation\nCette formule nécessite la mise en place d’une expérience\nDans laquelle moins une business unit va servir de groupe contrôle\nEt donc ne pas utiliser l’information dont on cherche à déterminer la valeur\nC’est aussi un indicateur “tardif” qui nous donne la différence de CA à posteriori\nTu l’as compris, il n’est pas parfait…\nMais il a le mérite de définir une formule pour valoriser les informations\nQui peut s’appliquer quelque soit l’information ou la société\nD’autres formules tel que l’Intrinsic Value of Information ou la Cost Value of Information sont décrites dans son livre Infonomics (que je recommande)\nEt tu as aussi 2 cours du même nom sur Coursera, qui sont accessible en auditeur libre 😉"
  },
  {
    "objectID": "blog/post_43.html",
    "href": "blog/post_43.html",
    "title": "Meme business case quantitatif",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_mod%C3%A9liser-les-b%C3%A9n%C3%A9fices-est-un-excellent-activity-7075426797563039745-DvDm?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Fri, 16 Jun 2023\n\nModéliser les bénéfices est un EXCELLENT moyen de créer de l’intérêt pour ton projet DATA.\nLe problème des business cases quali’ c’est qu’ils peuvent paraître parfois peu vraisemblables…\nOn se dit que si les bénéfices ne sont pas mesurables, est-ce qu’ils vont réellement faire une différence pour l’organisation ?\nPouvoir chiffrer les bénéfices du projet (même si ce n’est pas en €) plus de poids et rend le business case plus crédible"
  },
  {
    "objectID": "blog/post_45.html",
    "href": "blog/post_45.html",
    "title": "Valorisation des données",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataroi-datavaluation-datastrategy-activity-7079692406396268544-vaMe?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 28 Jun 2023\n\n4 approches pour valoriser une donnée ou une information !\n\nValorisation par les coûts\n\nOn va regarder ce que ça coûte de remplacer cette données en cas de perte ou les coûts de gestion/acquisition de cette donnée pour en déterminer la valeur\n\nValorisation intrinsèque\n\nLa valorisation intrinsèque va évaluer la valeur de la donnée de façon indépendante à l’utilisation actuelle. On va déterminer la valeur “absolue” de la données en utilisant différents facteurs comme la rareté, la couverture, la qualité, etc\n\nValorisation par le prix de marché\n\nRarement possible en pratique, nécessite qu’il existe un marché pour cette données et que ce marché “price” la données correctement. On va ensuite utiliser le prix de marché pour valoriser notre donnée\n\nValorisation par l’utilité\n\nOn va regarder à quel point cette donnée est utile pour déterminer sa valeur. Le plus elle impacte positivement les enjeux d’un individus ou d’une organisation le plus elle aura de valeur"
  },
  {
    "objectID": "blog/post_47.html",
    "href": "blog/post_47.html",
    "title": "Data-as-an-asset",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-dataasset-datastrategy-activity-7081930263563264002-xtP3?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 04 Jul 2023\n\nhttps://www.linkedin.com/posts/yacine-bekka_data-dataasset-datastrategy-activity-7081930263563264002-xtP3?utm_source=share&utm_medium=member_desktop\nPasser de “data-as-a-cost” à “data-as-an-asset”\nBeaucoup d’organisations voient encore les données comme un centre de coûts…\nElles n’exploitent pas le “vrai” pouvoir de la data.\nLes données servent uniquement à soutenir le bon fonctionnement des opérations.\nAu mieux, un reporting, un tableau de bord ici et là…\nLa vraie valeur de la data, c’est de pouvoir l’utiliser pour prédire un état futur.\nDans la vie et dans les affaires, l’immense majorité des décisions sont prises dans un état d’incertitude.\nParfois plus ou moins important…\nLa capacité à prédire le futur, même de façon imparfaite, constitue un avantage, puisqu’on vient réduire cette incertitude.\nPar exemple, si vous pouvez prédire les employés susceptibles de démissionner.\nCela va permettre de prendre de meilleures décisions opérationnelles et in fine :\n• d’améliorer la rétention, • de diminuer les coûts de recrutement, • d’éviter que des postes critiques pour l’organisation ne restent vacants trop longtemps.\nC’est de cette façon qu’une data devient une source de profit pour l’organisation."
  },
  {
    "objectID": "blog/post_49.html",
    "href": "blog/post_49.html",
    "title": "Carousel VEIPPP",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_valeur-esp%C3%A9r%C3%A9e-dune-information-partielle-activity-7080054792445685760-nII5?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Thu, 29 Jun 2023"
  },
  {
    "objectID": "blog/post_50.html",
    "href": "blog/post_50.html",
    "title": "Carousel VEII",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_valeur-esp%C3%A9r%C3%A9e-dune-information-imparfaite-activity-7079020492392652800-GE7B?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Mon, 26 Jun 2023"
  },
  {
    "objectID": "blog/post_52.html",
    "href": "blog/post_52.html",
    "title": "Carousel Espérance et decision",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_esp%C3%A9rance-de-gain-et-prise-de-d%C3%A9cision-activity-7076800858734096384–UAg?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 20 Jun 2023"
  },
  {
    "objectID": "blog/post_54.html",
    "href": "blog/post_54.html",
    "title": "Slides - Data quality introduction",
    "section": "",
    "text": "Vidéo Youtube : https://www.youtube.com/watch?v=vrQiPzKHoXQ\nDate originale de publication : 22 May 2023"
  },
  {
    "objectID": "blog/post_56.html",
    "href": "blog/post_56.html",
    "title": "Slides - Standard de qualité des données",
    "section": "",
    "text": "Vidéo Youtube : https://youtu.be/SIf8w5Fx6Cc\nDate originale de publication : 28 May 2023"
  },
  {
    "objectID": "blog/post_7.html",
    "href": "blog/post_7.html",
    "title": "Rules-based DQ control vs ML/AI based DQ control",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_dataquality-ai-chatgpt-activity-7023560922568777728-2HoB?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Tue, 24 Jan 2023\n\nAre you thinking about using AI for monitoring your hashtag#dataquality ? Let me share with you why it might not be the best approach\n🧠 AI-based control - Question answered : Is this record likely to be anomaly, based on all other existing records ? - Pros : Doesn’t require business SME/data producer involvement - Cons : Trickier to interpret (false positive/false negative), blind to rules not inferable from the dataset, depend on the quality of data itself\n📜 Manually-defined rules based control - Question answered : Does this record comply with the predefined rule for sure ? - Pros : Easily interpretable, what you see is what you get - Cons : Rules have to be defined manually, usually by business SME/data producer\nI would usually not recommend using only AI-based control (even if you work with big data), as you would miss a lot of data quality issues and have some records flagged as issues that are perfectly accurate.\nKeep in mind also that you either develop the hashtag#AI internally (which is usually costly) or have to rely on a vendor’s solution which would be a black box (if the solution is working fine, no problem but if it’s not…).\nIt can be a great tool to either complement manually defined rules or to avoid the “blank page syndrome” with business SMEs by having a set of rules to start the discussion (though other solutions exist for that purpose) but it doesn’t replace the “old-school” way of doing things !\nA final thought, with the recent improvement in natural language models (hashtag#chatgpt), you can imagine a data quality rules inference model that would take as an input not only the data but also the documentation around this data (business process documentation, user guide for IT tools, etc).\nThis kind of solution would come with its own set of problems (confidentiality of documents, internal dev vs vendor solution, etc) but one can dream about what might be the future of data quality management…. Humm, what was the definition of this indicator again? 😉"
  },
  {
    "objectID": "blog/post_9.html",
    "href": "blog/post_9.html",
    "title": "Data reliability",
    "section": "",
    "text": "Repost Linkedin : https://www.linkedin.com/posts/yacine-bekka_data-dataquality-digital-activity-7028995022901411840-COHT?utm_source=share&utm_medium=member_desktop\nDate originale de publication : Wed, 08 Feb 2023\n\nHave you already heard about “data reliability” ?\nI didn’t know about it either until I stumbled across it recently in my Linkedin feed so i looked into it :\n\nDo you trust your hashtag#data ?\nIf you ask them something will they tell the truth ?\nWill they be there when you need them the most ?\n\nAll of those aspects can be summarized by are your data reliable… 👏\n“I got it but isn’t it the same than hashtag#dataquality ?” 🧐\nAlmost, but with a small tweak.\nJust take data quality over a period of time and voilà… you have data reliability 📈\nAlso, the term is quite popular in the data engineering community but less so with the data management folks that will usually prefer data quality.\nData reliability is as important as data quality as it impacts all processes and tools that use data.\nWith reliable data you will increase the knowledge you have on your company and your market which will lead to better business decisions.\nWith unreliable data though, you will face issues with every hashtag#digital related initiatives.\nIt will prevents your company from having efficient operations and benefiting from the latest innovations in analytics and machine learning.\nDid you like this definition ?"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Pour me contacter, il vous suffit de m’envoyer un email à l’adresse suivante : contact@bekkaconsulting.com. Je vous répondrai dans les plus brefs délais."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Portfolio & ressources",
    "section": "",
    "text": "Agent autonome pour le jeu de Yathzee\n\nAgent autonome conçu pour jouer contre ou assister un joueur humain au jeu de Yahtzee. Cet agent utilise l’algorithme MCTS pour prendre des décisions en temps réel pendant le jeu, et atteint une moyenne de 154 points par partie.\nEntièrement implémenté côté client, les dépendances sont minimales, étant principalement développé en JavaScript “vanilla” avec HTML/CSS (à l’exception des graphiques, qui s’appuient sur Chart.js).\n\nPython library for statistical analysis (Work in progress)\n\nCollection de fonctions Python pour l’analyse statistique (tests statistiques, analyse de puissance). Cette bibliothèque nécessite NumPy pour le chargement des données et SciPy pour les fonctions liées aux lois de probabilité. Principalement destinée à un usage personnel pour l’instant, mais qui sait ce que l’avenir nous réserve…\n\nTS Proxy\n\nImplémentation d’un serveur de proxy HTTP/HTTPS/SOCKS5 avec des fonctionnalités additionnelles telles que la gestion de listes blanches/noires et des contre-mesures basiques contre les attaques DDoS."
  },
  {
    "objectID": "projects.html#software",
    "href": "projects.html#software",
    "title": "Portfolio & ressources",
    "section": "",
    "text": "Agent autonome pour le jeu de Yathzee\n\nAgent autonome conçu pour jouer contre ou assister un joueur humain au jeu de Yahtzee. Cet agent utilise l’algorithme MCTS pour prendre des décisions en temps réel pendant le jeu, et atteint une moyenne de 154 points par partie.\nEntièrement implémenté côté client, les dépendances sont minimales, étant principalement développé en JavaScript “vanilla” avec HTML/CSS (à l’exception des graphiques, qui s’appuient sur Chart.js).\n\nPython library for statistical analysis (Work in progress)\n\nCollection de fonctions Python pour l’analyse statistique (tests statistiques, analyse de puissance). Cette bibliothèque nécessite NumPy pour le chargement des données et SciPy pour les fonctions liées aux lois de probabilité. Principalement destinée à un usage personnel pour l’instant, mais qui sait ce que l’avenir nous réserve…\n\nTS Proxy\n\nImplémentation d’un serveur de proxy HTTP/HTTPS/SOCKS5 avec des fonctionnalités additionnelles telles que la gestion de listes blanches/noires et des contre-mesures basiques contre les attaques DDoS."
  },
  {
    "objectID": "projects.html#long-form-content",
    "href": "projects.html#long-form-content",
    "title": "Portfolio & ressources",
    "section": "Long-form content",
    "text": "Long-form content\n\nRapport technique - Agent autonome pour le jeu de Yathzee\n\nRapport technique détaillant le fonctionnement de l’agent autonome pour le jeu de Yahtzee.\n\nGuide to Data ROI, 1st edition\n\nGuide sur la manière d’estimer le ROI des projets data. Publié en 2023, la première édition n’est plus disponible publiquement en attendant la publication de la deuxième édition, que j’espère sortir bientôt !\nCela dit, si vous êtes vraiment intéressé, n’hésitez pas à m’envoyer un e-mail ;)"
  }
]